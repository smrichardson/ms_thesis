\documentclass{article}

% packages from original
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{csquotes}
\usepackage[nameinlink]{cleveref}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{thm-restate}

% theorems and formatting
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

% margins
\usepackage[margin=1in]{geometry}

% line spacing
\renewcommand{\baselinestretch}{1.5}

% define \E as \mathbb{E}
\newcommand{\E}{\mathbb{E}}
\newcommand{\EE}{\mathbb{E}}

% define \equaldist, \dist for distributions and equality
\newcommand{\equaldist}{\stackrel{d}{=}}
\newcommand{\dist}{\stackrel{d}{\sim}}

\usepackage{subfiles}

\begin{filecontents}{cover.tex}
    \documentclass[12pt]{article}
    \topmargin= -0.4in
    \textheight = +8.9in
    \oddsidemargin = 0.05in
    \evensidemargin = 0.05in
    \textwidth = 6.5in
    \usepackage{amssymb}
    
    \begin{document}
    
    \noindent
    \thispagestyle{empty}
    \underline{\bf Master's Paper of the Department of Statistics, The University of Chicago}
    %\\~~(Internal document only, not for circulation) 
    \\~~(Internal departmental document only, not for circulation. Anyone wishing to publish or cite any portion therein must have the express, written permission of the author.)
    
    \vspace{1.8in}
    \begin{center}
    {\bf\LARGE Degree Paper for Masters in Statistics}
    \\~\\
    {\bf\Large --- A Sample Format}
    
    
    \vspace{1.4in}
    {\Large Sean Richardson}
    
    \vspace{1.3in}
    {\Large Advisor: Victor Veitch}
    
    \end{center}
    
    \vspace{.6in}
    {\Large Approved} ~\underline{~~~~~~~~~~~~~~~~~~
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~}
    
    \vspace{.2in}
    {\Large Date} ~\underline{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}
    
    \vfill
    \begin{center}{\large January 5, 2025}\end{center}
    \end{document}
\end{filecontents}

\begin{filecontents}{body.tex}

\newpage
\begin{abstract}
This thesis concerns the statistical evaluation of reward models used in language modeling. A reward model is a function that takes a prompt and a response and assigns a score indicating how ``good'' that response is for the prompt. A key challenge is that reward models are usually imperfect proxies for actual preferences. For example, we may worry that a model trained to reward helpfulness learns to instead prefer longer responses. 

In this thesis, we develop an evaluation method, \emph{RATE} (Rewrite-based Attribute Treatment Estimators), that allows us to measure the \emph{causal} effect of a given attribute of a response (e.g., length) on the reward assigned to that response. The core idea is to use large language models (LLMs) to \emph{rewrite} responses to produce approximate counterfactuals, and to adjust for rewriting error by rewriting \emph{twice}. We prove $\sqrt{n}$-consistency of the estimator under reasonable assumptions and demonstrate its effectiveness empirically. This work extends classical causal inference techniques to handle the unique challenges posed by text data in the context of reward modeling.

\textit{This thesis is based on joint work with David Reber, Todd Nief, Cristina Garbacea, and Victor Veitch. The statistical theory, consistency proofs, and methodological extensions presented here represent my primary contributions to this collaboration.}
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}
\label{sec:intro}

\subsection{Motivation}

In the context of large language models (LLMs), reward models evaluate the quality or appropriateness of model outputs, either by assessing individual responses or comparing multiple alternatives. Such models are useful in a variety of settings, including alignment of LLMs, ranking output samples (e.g., to use in a best-of-$n$ sampling procedure), or evaluating LLM performance in a stable, automated way.

Ideally, reward models would perfectly capture and measure whatever aspect of the output is important---for example, a reward model for mathematical problem solving would precisely identify whether the generated response is correct. However, reward models are commonly learned from training data that imperfectly measures more nebulous attributes. For instance, a frequent approach is to train a reward model based on human pairwise preferences for which of two responses is ``more helpful.'' This may lead the reward model to conflate \emph{helpfulness} with other textual attributes it happens to correlate with (e.g., length, style, politeness).

\textbf{The Key Challenge.} Even if we have a reward model, it remains unclear what the model is \emph{actually} rewarding. As a result, we risk deploying or further training a model on a misaligned or spurious proxy. For instance, a ``helpfulness'' reward model could inadvertently reward longer responses, regardless of their true helpfulness.

\subsection{Statistical Challenges in Text-Model Evaluation}
Language models introduce unique statistical challenges for causal inference:
\begin{enumerate}
    \item \textbf{High Dimensionality and Complexity:} Text is extremely high-dimensional and unstructured. Multiple textual attributes may be correlated in ways that are difficult to disentangle.
    \item \textbf{Spurious Correlations:} Since human annotation is limited and often noisy, the training data may reveal spurious relationships (e.g., length correlates with perceived helpfulness).
    \item \textbf{Potential for Bias:} Any text attribute of interest (sentiment, politeness, helpfulness, complexity) might be confounded with other attributes in the data, making purely observational methods insufficient.
\end{enumerate}

\textbf{A Naive Approach.} One might try to isolate the impact of an attribute $W$ by simply partitioning the data into $W=1$ and $W=0$ and comparing mean reward. However, if $W$ is correlated with other features of the text, that naive approach will conflate multiple causal effects, failing to isolate the \emph{direct} effect of $W$.

\subsection{Our Contribution}
To properly estimate how the reward would change if we were to \emph{intervene} on one specific text attribute while keeping the other text aspects fixed, we adopt a formal causal perspective. This thesis introduces \textbf{RATE (Rewrite-based Attribute Treatment Estimators)}, a novel approach that uses large language models (LLMs) to rewrite text into approximate counterfactuals. These counterfactuals differ only in one target attribute (e.g., length, sentiment, helpfulness), leaving other attributes as intact as possible. 

However, LLM-based rewrites are typically \emph{imperfect}, and may inadvertently alter additional attributes. To correct for these off-target changes, we propose rewriting \emph{twice} and comparing the resulting reward differences. Under mild assumptions, this procedure yields \emph{unbiased,} $\sqrt{n}$-consistent estimates of the average treatment effect (ATE), average treatment effect on the treated (ATT), and average treatment effect on the untreated (ATU).

\subsection{Structure of this Thesis}
In \Cref{sec:related_work}, we situate RATE among other causal approaches and reward-model evaluation methods. In \Cref{sec:setup}, we formalize the problem setup, introducing notation and the concept of causal estimands in the text domain. \Cref{sec:rate} describes our main algorithm, RATE, and \Cref{sec:theory} presents theoretical justifications. \Cref{sec:experiments} outlines empirical studies, highlighting both real-world and synthetic evaluations. Finally, \Cref{sec:discussion} covers extensions such as contrastive rewards and model edits, and \Cref{sec:conclusion} presents broader conclusions and future directions.

\section{Related Work}
\label{sec:related_work}
%% Added "Related Work" section as suggested

There is growing interest in understanding and mitigating biases or spurious correlations in NLP tasks, including reward modeling:

\begin{itemize}
    \item \textbf{Causal Inference in NLP.} Researchers have explored causal-inference frameworks for text, especially for text classification \cite{feder2022causalinferencenaturallanguage}. \emph{RATE} extends these ideas to \emph{reward models}, which have a similar structure but differ in the sense that they are often used for reinforcement learning or ranking scenarios.
    \item \textbf{Counterfactual Generation.} Generating minimal textual edits to isolate one attribute is an active research area. Systems like Polyjuice \cite{wu2021polyjuicegeneratingcounterfactualsexplaining} create diverse counterfactuals, but often do not explicitly correct for rewriting artifacts. CEBaB \cite{abraham2022cebab} manually constructs counterfactuals to isolate sentiment or other aspects, though it requires significant human effort. RATE automatically generates and \emph{corrects} rewrites for spurious changes.
    \item \textbf{Reward Modeling and Evaluation.} \cite{lambert2024rewardbenchevaluatingrewardmodels} propose large-scale benchmarks for reward models without taking a causal perspective. \cite{park2024offsetbias} addresses the presence of certain biases (e.g., length) in reward models but does not use causal methods to estimate such biases. RATE aims to \emph{quantify} and isolate these biases by a principled causal procedure.
\end{itemize}

Overall, RATE can be viewed as an extension of text-oriented causal inference methods to address the unique demands of reward modeling—specifically, we focus on rewriting text in a way that yields an \emph{unbiased} measure of how changing an attribute will shift the reward.

\section{Problem Setup and Causal Estimands}
\label{sec:setup}

Reward models are typically implemented in two ways:

\begin{enumerate}
  \item \textbf{Pointwise:} $R(x, y)$ takes a prompt $x$ and a response $y$ and returns a scalar reward.  
  \item \textbf{Contrastive:} $R(x, y_1, y_0)$ takes a prompt $x$ and two responses $y_1, y_0$, returning a scalar indicating which response is better (or by how much).
\end{enumerate}

Our derivations hold for both, but we focus on the pointwise version to keep notation simple. Let $\{(x^i, y^{ij})\}$ be a dataset of prompt-response pairs, with $R(x^i, y^{ij}) \in \mathbb{R}$.

We consider a binary attribute $W(x^i, y^{ij}) \in \{0,1\}$ (e.g., ``sentiment is positive'' or ``response is long'') that we hypothesize may affect the reward. For instance, in a user-query dataset, $W$ might indicate whether the response is helpful for that query.

\subsection{Naive Estimation and Its Pitfall}
An obvious approach to see if $W$ influences $R$ is to compare the average reward for $W=1$ vs. $W=0$:
\[
\hat{\tau}_{\text{naive}} 
= \frac{1}{n_1}\sum_{i,j: w^{ij}=1} R(x^i, y^{ij})
- \frac{1}{n_0}\sum_{i,j: w^{ij}=0} R(x^i, y^{ij}).
\]
However, this fails to isolate the causal effect if $W$ is correlated with other confounding attributes. For example, if $W=1$ responses are systematically longer or more detailed in ways that also raise their reward, we cannot infer whether the reward model truly favors $W$ (versus favoring those other correlated attributes).

\subsection{Treatment Effects}
To disentangle such confounders, we adopt a causal-inference framework. For each $(x, y)$, suppose there exist two \emph{potential responses}:
\[
Y(0), \quad Y(1),
\]
which are identical except that one has $W=0$ and the other has $W=1$. The \emph{average treatment effect} (ATE) is:
\[
\text{ATE} 
= \E\bigl[R(X, Y(1)) - R(X, Y(0))\bigr].
\]
We also define:
\[
\text{ATT} = \E\bigl[R(X, Y(1)) - R(X, Y(0)) \;\big|\; W=1\bigr],
\]
\[
\text{ATU} = \E\bigl[R(X, Y(1)) - R(X, Y(0)) \;\big|\; W=0\bigr].
\]
In principle, $Y(1)$ and $Y(0)$ are never simultaneously observed. We only see whichever label $W$ the data happened to have. This is the fundamental problem of causal inference \cite{imbens2015causal}.

\subsection{Why Not Simple Backdoor Adjustments?}
\label{sec:why_not_backdoor}
A common approach to causal inference is to apply \emph{backdoor adjustments} (e.g., regression, matching, inverse propensity weighting) by conditioning on all confounders that causally affect both the treatment and the outcome \cite{cinelli2024crash}. However, in high-dimensional text settings, this approach faces critical barriers:

\begin{itemize}
    \item \textbf{Unknown Causal Structure.} Language is unstructured and can contain innumerable latent factors (sentiment, style, topic, complexity, speaker identity, etc.) whose causal roles are often unclear. Determining which textual features truly act as confounders is therefore nontrivial, if not infeasible, without extensive domain knowledge or perfect annotation of these features.

    \item \textbf{Context-Dependence.} Even if we attempt to identify potential confounders (e.g., style, topic), these may vary across documents and domains. A feature that is a confounder in one context (e.g., sentiment in a product review) may be irrelevant in another (e.g., a factual Q\&A). Consequently, there is no universal set of textual covariates we can reliably adjust for.

    \item \textbf{Combinatorial Explosion.} In principle, we could embed each text into a high-dimensional representation (e.g., word embeddings or bag-of-words) and treat each dimension as a covariate for regression or propensity modeling. However, this quickly becomes computationally intractable and statistically fragile (``curse of dimensionality''), especially when sample sizes are finite. This also introduces overlap issues, where the covariates may have different supports across treatment groups.

\end{itemize}

In contrast, a \emph{rewrite-based} approach such as \textbf{RATE} circumvents the need to explicitly identify and condition on all relevant confounders. Instead, it leverages the power of LLMs to generate counterfactuals that are as close as possible to the desired treatment, while keeping other aspects of the text fixed.

\section{RATE: Rewrite-based Attribute Treatment Estimators}
\label{sec:rate}

\subsection{Rewrite Operations with LLMs}
To approximate the counterfactual $Y(1)$ for a text that actually had $W=0$, we ask an LLM to \emph{rewrite} $y$ to have $W=1$ while leaving everything else the same. Denote:
\[
\text{Re}(x^i, y^{ij}, w_{\text{new}}) \quad\longrightarrow\quad \tilde{y}^{ij},
\]
where $\tilde{y}^{ij}$ is the LLM-generated rewrite of $y^{ij}$ that attempts to enforce $W(\tilde{y}^{ij}) = w_{\text{new}}$ but otherwise preserve the rest of the text. An example instruction might be:  
\begin{quote}
``Rewrite the following text to be more helpful, \textit{without} changing other aspects such as tone, sentiment, or factual content.''  
\end{quote}

\subsection{Imperfect Rewrites and the ``Double Rewrite'' Idea}

\textbf{Challenge:} LLM rewrites can inadvertently alter off-target attributes (e.g., grammar, length, writing style). Let $\epsilon_w$ be the difference in the reward caused by these unintended changes. If we compare
\[
R(x, y) - R\bigl(x, \text{Re}(x,y,0)\bigr),
\]
we pick up the effect of rewriting itself, not purely the effect of the attribute.

\textbf{Solution:} Use \emph{Rewrite-of-Rewrite}. Instead of comparing an original $y$ to one rewrite, we:
\begin{itemize}
    \item Start with $y$ that has $W=1$,
    \item Rewrite it to $W=0$, then
    \item Rewrite that rewrite back to $W=1$.
\end{itemize}
Symbolically, 
\[
\text{Re}\bigl(x,\text{Re}(x,y,0),1\bigr),
\]
is the \emph{rewrite-of-rewrite} from $W=0$ back to $W=1$. Intuitively, the textual artifacts introduced by rewriting once are also introduced by rewriting again, so they can cancel out in expectation. Hence the \textbf{RATE} estimator compares:
\[
R\Bigl(x,\;\text{Re}(x, \text{Re}(x,y,0),1)\Bigr) \;-\;
R\Bigl(x,\;\text{Re}(x,y,0)\Bigr),
\]
when the \emph{original} $y$ has $W=1$. Similarly, if $W=0$, we compare $R(\text{Re}(x,y,1))$ to $R(\text{Re}(x, \text{Re}(x,y,1),0))$. We can define the RATE estimation procedure as follows:
\begin{algorithm}[H]
  \caption{RATE: Rewrite-based Attribute Treatment Estimators}
  \label{alg:rate}
  \begin{algorithmic}[1]
  \State \textbf{Input:} Dataset $\{(x^i, y^{ij}, w^{ij})\}$, reward model $R$, rewrite function $\text{Re}$
  \State \textbf{Return:} Estimates $\widehat{\text{ATT}}_{\text{RATE}}$, $\widehat{\text{ATU}}_{\text{RATE}}$, and $\widehat{\text{ATE}}_{\text{RATE}}$
  \State \textbf{Compute:} $n_1 = \sum_{i,j} \mathbf{1}[w^{ij}=1], \quad n_0 = \sum_{i,j} \mathbf{1}[w^{ij}=0]$
  \State \textbf{For all }(i,j): \textbf{obtain rewrites} $\text{Re}(x^i, y^{ij}, 0)$ if $w^{ij = 1}$ and $\text{Re}(x^i, y^{ij}, 1)$ if $w^{ij} = 0$
  \State \textbf{For all rewrites:} generate a rewrite-of-rewrite in the opposite direction
  \State \textbf{ATT:}
  \[
    \widehat{\text{ATT}}_{\text{RATE}} = \frac{1}{n_1} \sum_{(i,j)\colon w^{ij}=1} \Bigl[\,R\bigl(x^i, \text{Re}(x^i, \text{Re}(x^i, y^{ij},0),1)\bigr) - R\bigl(x^i,\text{Re}(x^i, y^{ij},0)\bigr)\Bigr].
  \]
  \State \textbf{ATU:}
  \[
    \widehat{\text{ATU}}_{\text{RATE}} = \frac{1}{n_0} \sum_{(i,j)\colon w^{ij}=0} \Bigl[\,R\bigl(x^i, \text{Re}(x^i, y^{ij},1)\bigr) - R\bigl(x^i,\text{Re}(x^i, \text{Re}(x^i, y^{ij},1),0)\bigr)\Bigr].
  \]
  \State \textbf{ATE: }
  \[
    \widehat{\text{ATE}}_{\text{RATE}} = 
    \frac{n_1}{n_1+n_0}\,\widehat{\text{ATT}}_{\text{RATE}}
    \;+\;
    \frac{n_0}{n_1+n_0}\,\widehat{\text{ATU}}_{\text{RATE}}.
  \]
  \State \Return $\widehat{\text{ATT}}_{\text{RATE}}, \,\widehat{\text{ATU}}_{\text{RATE}}, \,\widehat{\text{ATE}}_{\text{RATE}}$
  \end{algorithmic}
\end{algorithm}

\section{Theoretical Analysis}
\label{sec:theory}

We show that RATE is both unbiased and $\sqrt{n}$-consistent under mild assumptions.

\subsection{Latent Variable Model and Assumptions}

Consider representing each response $Y$ as $Y(W, Z, \xi)$, where:
\begin{itemize}
    \item $W$ is the target binary attribute
    \item $Z$ are off-target attributes that remain \emph{invariant} under rewriting
    \item $\xi$ are off-target attributes that \emph{may} change inadvertently under rewriting
\end{itemize}

We then assume:

\begin{assumption}[Additive Reward Decomposition]
\label{assump:additive}
\[
R\bigl(X,\;Y(W,Z,\xi)\bigr) = R_{W,Z}(X,W,Z) \;+\; R_{\xi}(X,\xi).
\]
\end{assumption}

\begin{assumption}[Rewrite Distribution]
\label{assump:rewrite_dist}
Rewriting $Y(W,Z,\xi)$ to enforce $W=w_{\text{new}}$ yields a distribution over $\xi$ values independent of $(W,Z)$. Formally,
\[
\text{Re}\bigl(X, Y(W,Z,\xi), w_{\text{new}} \bigr) \;\stackrel{d}{=}\; Y\bigl(w_{\text{new}},Z,\widetilde{\xi}\bigr),
\quad\text{where } \widetilde{\xi} \sim \mathbb{P}_{\mathrm{Re}}(\widetilde{\xi}).
\]
\end{assumption}
\noindent
\Cref{assump:additive} states that $\xi$ (mutable attributes such as stylistic artifacts) \emph{add} to the reward in a way that does not interact with $W$ or $Z$. \Cref{assump:rewrite_dist} posits that rewriting is essentially sampling a new $\xi$ from some distribution $\mathbb{P}_{\mathrm{Re}}$, representing the random off-target changes that an LLM might introduce.

Under these assumptions, we have:

\begin{restatable}[Unbiasedness and Consistency]{theorem}{mainthm}
\label{thm:mainthm}
Let $R(X, Y(W,Z,\xi)) = R_{W, Z}(X, W, Z) + R_{\xi}(X, \xi)$ and $\text{Re}(Y(W, Z, \xi), 1 - W) \equaldist Y(1 - W, Z, \tilde{\xi})$ where $\tilde{\xi} \dist P_\text{Re}(\tilde{\xi})$. Also assume $R(\cdot, \cdot)$ is bounded. Suppose we have a set of prompt-completion pairs $\{x^i, y^{ij}\}$ sampled i.i.d., with $P(W=1) \in (0, 1)$.
  Then the RATE estimators, defined as:
  \begin{align*}
  \widehat{\text{ATT}}_{\text{RATE}} &= \frac{1}{n_1} \sum_{(i, j): w^{ij} = 1} [R(x^i, \text{Re}(\text{Re}(y^{ij}, 0), 1)) - R(x^i, \text{Re}(y^{ij}, 0))] \\
  \widehat{\text{ATU}}_{\text{RATE}} &= \frac{1}{n_0} \sum_{(i,j): w^{ij} = 0} [R(x^i, \text{Re}(y^{ij}, 1)) - R(x^i, \text{Re}(\text{Re}(y^{ij}, 1), 0))] \\
  \widehat{\text{ATE}}_{\text{RATE}} &= \frac{n_1}{n_0 + n_1} \widehat{\text{ATT}}_{\text{RATE}} + \frac{n_0}{n_0 + n_1} \widehat{\text{ATU}}_{\text{RATE}}
\end{align*}
where $n_1$ and $n_0$ are the number of pairs with observed $W = 1$ and $W = 0$ respectively, are unbiased and $\sqrt{n}$-consistent estimators of the ATT, ATU, and ATE.
\end{restatable}

See \Cref{sec:proofs} for a complete proof.

\subsection{Intuitive Explanation of the Proof}
The double-rewrite operation effectively ensures that the random off-target effects introduced in rewriting cancel out in expectation. By rewriting $W=1$ text to $W=0$ and then rewriting back to $W=1$, we stay in the same ``rewrite distribution space'' and thus keep the off-target $v$ artifacts aligned in expectation. Consequently, the difference in rewards isolates the effect of going from $W=0$ to $W=1$. A similar argument applies for going from $W=1$ to $W=0$. 

\paragraph{Justifying \Cref{assump:additive}:} Intuitively, human preferences for many attributes are separable. For example, the strength of our preference for a response to be helpful ($W$) is unlikely to depend on attributes like the specific wording used $(\xi)$. Rewards, then, as approximations of human preferences, should also be separable in this way. To be sure, such separability does not, intuitively, hold in some cases (e.g., the strength of our preference for a response to be cheerful may depend on the topic of the response), but these cases seem to involve immutable attributes $Z$ rather than mutable attributes $\xi$, at least when we are considering rewrites done by sophisticated LLMs, as they will not change the topic of a response when asked to change its sentiment.

\paragraph{Justifying \Cref{assump:rewrite_dist}:} LLMs have a characteristic way of writing that appears to consistent across contexts. For instance, LLMs will avoid using profanity or grammatical errors in most contexts. Thus, it is plausible that the distribution of $\xi$ values introduced by rewriting is independent of the original $W, Z$ value (though this is ultimately an empirical question likely to vary across LLMs).

\section{Empirical Evaluation}
\label{sec:experiments}
%% Expanded discussion of experiments

In this section, we outline how RATE is applied in practice and illustrate its efficacy on both real-world and synthetic data. We focus on a brief summary here; for more extensive results, see the original paper on which this thesis is based.

\subsection{Implementation Details}

\textbf{LLM-Based Rewriting.} We use an LLM (e.g., GPT-4) to generate rewrites in a batch manner. For each sample $(x^i, y^{ij}, w^{ij})$, we:
\begin{enumerate}
    \item For the examples with $w^{ij} = 0$, prompt the LLM: \emph{``Rewrite this response so that [attribute] = 1 while changing nothing else.''}
    \item Similarly, for the examples with $w^{ij} = 1$, prompt: \emph{``Rewrite this response so that [attribute] = 0 while changing nothing else.''}
    \item For each of these two rewrites, do a second rewrite to flip the attribute back.
\end{enumerate}

\textbf{Example Prompt.} Suppose $W$ is \emph{sentiment}, and the original text is negative. We instruct:  
\begin{quote}
\textit{``Rewrite the following text to make it \textbf{positive in sentiment}, but do not change anything else like factual content or style.''}
\end{quote}

\textbf{Handling Edge Cases.} In some datasets, an example might already exhibit a contradictory combination of attributes (e.g., negative text that also has local positivity). We rely on the LLM’s general handling, but do manually inspect a small subset of rewrites to ensure they are not totally off.

\textbf{Computational Cost.} Each rewrite is a forward pass through an LLM. For 25K examples, generating rewrites and rewrites-of-rewrites can cost tens of dollars with current APIs, which is acceptable for research-scale experiments but may be expensive at higher volume. At any rate, the cost is much lower than the cost of collecting human-generated counterfactuals.

Once the rewrites are generated, we pass them through a reward model, yielding the estimates $\widehat{\text{ATT}}_{\text{RATE}}$, $\widehat{\text{ATU}}_{\text{RATE}}$, and $\widehat{\text{ATE}}_{\text{RATE}}$.

\subsection{Real-World Data and Observations}

We apply RATE to real-world reward models and real-world data. We observe several key phenomena:

\textbf{Comparison to Naive Estimator.} We see large discrepancies between the naive estimates of the effect of $W$ vs. the RATE-based estimates. In many settings, naive methods suggest the reward model strongly ``favors'' length, but RATE reveals a substantially smaller effect, indicating that length might have been correlated with truly helpful content.

\textbf{Rewrite-of-Rewrite Necessity.} We also compare a \emph{single-rewrite} approach (comparing $y$ to $\text{Re}(x, y, \bar{w})$) to the double rewrite. The single-rewrite approach consistently yields biased estimates that overstate the effect of $W$. The difference can be substantial---on some tasks, the sign of the effect even flips.

\subsection{Synthetic Experiments}

In a semi-synthetic setup, we synthesize text with known $W$ attributes (positive vs. negative, short vs. long) and artificially correlate $W$ with another attribute. We then use a black-box reward model (e.g., a sentiment classifier). Since we know the ground truth correlation structure, we can test how each estimator behaves under distributional shifts:
\begin{itemize}
    \item \textbf{Naive Estimator} exhibits large changes in reported effect whenever the correlation is increased or decreased.
    \item \textbf{RATE} remains stable and aligned with the (assumed) ATE across different correlation levels, validating the causal interpretation.
\end{itemize}

Overall, these empirical studies show that RATE is both practical (easy to implement with current LLM APIs) and significantly more robust than naive alternatives.

\section{Discussion and Limitations}
\label{sec:discussion}

\paragraph{Generalization to Contrastive Rewards.}
As hinted in \Cref{sec:setup}, RATE naturally extends to $R(x, y_1, y_0)$ by looking at pairs of rewrites in the contrastive setting. The result is an estimate of
\[
\E\bigl[R(X,\,Y(1),\,Y(0))\bigr],
\]
i.e., how changing one attribute in isolation of everything else affects the model's \emph{relative} preference.

\paragraph{Model Edits and Steering.}
An interesting extension is to compare two different models, $\pi$ vs. $\pi_0$, using:
\[
\tilde{R}(x,y) = \log\frac{\pi(y|x)}{\pi_0(y|x)}.
\]
This can reveal whether a fine-tuned or otherwise modified model truly changed its behavior on attribute $W$, again controlling for off-target shifts. This is particularly useful for evaluating ``steering vectors'' $\lambda$ where $\lambda$ is added to the residual stream of a transformer-based LLM, inducing the distribution $\pi_\lambda$. RATE can help determine whether $\lambda$ truly steers the model in the intended direction.

\paragraph{Limitations.}
Despite its benefits, RATE relies on:
\begin{itemize}
    \item \textbf{Quality of LLM Rewrites.} If rewriting instructions are misunderstood or if the LLM is unwilling to produce certain rewrites (e.g., making text intentionally unhelpful), it can be challenging to gather valid pairs.
    \item \textbf{No Guarantee of Perfect Attribute Control.} We assume rewrites yield texts that differ \emph{only} in $W$, but LLMs may drift in subtle ways.
    \item \textbf{Additive Decomposability.} While plausible for many aspects of human preferences, the additive form of $R$ (\Cref{assump:additive}) is not guaranteed in every scenario.
\end{itemize}

\section{Conclusion and Future Work}
\label{sec:conclusion}

This thesis introduced \textbf{RATE (Rewrite-based Attribute Treatment Estimators)}, a method for estimating the causal effect of a particular textual attribute on a reward model’s outputs. By leveraging LLMs to generate approximate counterfactuals and offsetting their imperfections via double rewriting, RATE delivers a principled and practical way to evaluate whether a reward model \emph{truly} responds to an attribute or merely exploits spurious correlations.

\textbf{Directions for Future Research.}
\begin{itemize}
    \item \emph{Enhancing Rewrite Quality.} Improvements in prompt design, or specialized rewrite models, may yield even better (i.e., more truly ``counterfactual'') text pairs.
    \item \emph{Prompt-Rewriting vs. Response-Rewriting.} Instead of rewriting the responses, one could rewrite the \emph{prompts}, then generate responses with or without attribute $W$. This approach might remove some complexities but introduces others.
    \item \emph{Beyond Binary Attributes.} Many attributes of interest are continuous or multi-class (e.g., a politeness scale). Extending RATE to these cases is an intriguing generalization.
\end{itemize}

Overall, RATE paves the way for more causally rigorous auditing of reward models, ensuring that alignment and preference models do not drift toward unintended or spurious attributes.

\newpage
\appendix
\section{Proof of Main Theorem}
\label{sec:proofs}
\begin{theorem}[Unbiasedness and \(\sqrt{n}\)-Consistency of RATE---Consistent Notation]
\label{thm:unbiased_rate_consistent}
Let $R(X, Y(W,Z,\xi)) = R_{W, Z}(X, W, Z) + R_{\xi}(X, \xi)$ and $\text{Re}(Y(W, Z, \xi), 1 - W) \equaldist Y(1 - W, Z, \tilde{\xi})$ where $\tilde{\xi} \dist P_\text{Re}(\tilde{\xi})$. Also assume $R(\cdot, \cdot)$ is bounded. Suppose we have a set of prompt-completion pairs $\{x^i, y^{ij}\}$ sampled i.i.d., with $P(W=1) \in (0, 1)$.
Then the RATE estimators, defined as:
\begin{align*}
\widehat{\text{ATT}}_{\text{RATE}} &= \frac{1}{n_1} \sum_{(i, j): w^{ij} = 1} [R(x^i, \text{Re}(\text{Re}(y^{ij}, 0), 1)) - R(x^i, \text{Re}(y^{ij}, 0))] \\
\widehat{\text{ATU}}_{\text{RATE}} &= \frac{1}{n_0} \sum_{(i,j): w^{ij} = 0} [R(x^i, \text{Re}(y^{ij}, 1)) - R(x^i, \text{Re}(\text{Re}(y^{ij}, 1), 0))] \\
\widehat{\text{ATE}}_{\text{RATE}} &= \frac{n_1}{n_0 + n_1} \widehat{\text{ATT}}_{\text{RATE}} + \frac{n_0}{n_0 + n_1} \widehat{\text{ATU}}_{\text{RATE}}
\end{align*}
where $n_1$ and $n_0$ are the number of pairs with observed $W = 1$ and $W = 0$ respectively, are unbiased and $\sqrt{n}$-consistent estimators of the ATT, ATU, and ATE.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:unbiased_rate_consistent}]
    First, we'll prove the unbiasedness and $\sqrt{n}$-consistency of $\widehat{\text{ATT}}_{\text{RATE}}$. The argument for $\widehat{\text{ATU}}_{\text{RATE}}$ follows by symmetry. Then, we can use these results to prove the same for $\widehat{\text{ATE}}_{\text{RATE}}$. Throughout, we use $\tilde{\xi}$ and $\tilde{\tilde{\xi}}$ to denote i.i.d. samples from the distribution $P_{\text{Re}}$, where the former comes from the first rewrite and the latter from the rewrite of the rewrite.
    
    \textbf{1. Unbiasedness and $\sqrt{n}$-Consistency of $\widehat{\text{ATT}}_{\text{RATE}}$}
    Fix a prompt $x$ and response $y$ with $w = 1$, omitting superscripts for convenience. Then by our latent variable model, $y = Y(1, z, v)$ for some realizations $z$ and $v$ of $Z$ and $\xi$. We calculate:
    \[R(x, \text{Re}(\text{Re}(y, 0), 1)) - R(x, \text{Re}(y, 0))\]
    which has expected value:
    \begin{align*}
    \EE_{\tilde{\xi}, \tilde{\tilde{\xi}} \sim P_{\text{Re}}}[R(x, y(1, z, \tilde{\tilde{\xi}})) - R(x, y(0, z, \tilde{\xi}))]
    &= \EE_{\tilde{\xi}, \tilde{\tilde{\xi}} \sim P_{\text{Re}}}[R_{W,Z}(x, 1, z) + R_\xi(x, \tilde{\tilde{\xi}})] \\ &- \EE_{\tilde{\xi}, \tilde{\tilde{\xi}} \sim P_{\text{Re}}}[R_{W,Z}(x, 0, z) + R_\xi(x, \tilde{\xi})] \\
    &= R_{W,Z}(x, 1, z) - R_{W,Z}(x, 0, z) \\
    &= R_{W,Z}(x, 1, z) - R_{W,Z}(x, 0, z) \\ &+ R_\xi(x, v) - R_\xi(x, v) \\
    &= R(x, y(1, z, v)) - R(x, y(0, z, v)) \\
    &= R(x, y(1)) - R(x, y(0))
    \end{align*}
    Therefore, as an average over these quantities, we have:
    \[\EE[\widehat{\text{ATT}}_{\text{RATE}}] = \EE[R(X, Y(1)) - R(X, Y(0)) | W = 1] = \text{ATT}\]
    For $\sqrt{n}$-consistency, note that $R(\cdot, \cdot)$ is bounded, so its variance is bounded. As the ${x^i, y^{ij}}$ are i.i.d., so are the $R(x^i, y^{ij})$. Thus, $\widehat{\text{ATT}}_{\text{RATE}}$ is an average over $n_1$ i.i.d. random variables with finite variance, implying:
    \[\sqrt{n_1}(\widehat{\text{ATT}}_{\text{RATE}} - \text{ATT}) = O_p(1)\]
    Since $\frac{n_1}{n} \xrightarrow{p} P(W=1)$ and $P(W=1) \in (0,1)$, we have $\sqrt{\frac{n}{n_1}} = O_p(1)$, which implies:
    \[\sqrt{n}(\widehat{\text{ATT}}_{\text{RATE}} - \text{ATT}) = O_p(1)\]
    
    \textbf{2. Unbiasedness and $\sqrt{n}$-Consistency of $\widehat{\text{ATU}}_{\text{RATE}}$}
    By the same argument as for ATT and since $P(W=0) \in (0,1)$:
    \[\EE[\widehat{\text{ATU}}_{\text{RATE}}] = \EE[R(X, Y(1)) - R(X, Y(0)) | W = 0] = \text{ATU}\]
    and
    \[\sqrt{n}(\widehat{\text{ATU}}_{\text{RATE}} - \text{ATU}) = O_p(1)\]
    
    \textbf{3. Unbiasedness and $\sqrt{n}$-Consistency of $\widehat{\text{ATE}}_{\text{RATE}}$}
    The ATE estimator is a weighted average of the ATT and ATU estimators. By the law of total expectation:
    \begin{align*}
    \EE[\widehat{\text{ATE}}_{\text{RATE}}] &= \EE[R(X, Y(1)) - R(X, Y(0)) | W = 1] \cdot P(W = 1) \\
    &+ \EE[R(X, Y(1)) - R(X, Y(0)) | W = 0] \cdot P(W = 0) \\
    &= \EE[R(X, Y(1)) - R(X, Y(0))] = \text{ATE}
    \end{align*}
    For $\sqrt{n}$-consistency, we can write:
    \begin{align*}
    \sqrt{n}(\widehat{\text{ATE}}_{\text{RATE}} - \text{ATE}) &= \frac{n_1}{n}\sqrt{n}(\widehat{\text{ATT}}_{\text{RATE}} - \text{ATT}) + \frac{n_0}{n}\sqrt{n}(\widehat{\text{ATU}}_{\text{RATE}} - \text{ATU})
    \end{align*}
    Since:
    \[\frac{n_1}{n} \xrightarrow{p} P(W=1), \frac{n_0}{n} \xrightarrow{p} P(W=0)\]
    \[\sqrt{n}(\widehat{\text{ATT}}_{\text{RATE}} - \text{ATT}) = O_p(1), \sqrt{n}(\widehat{\text{ATU}}_{\text{RATE}} - \text{ATU}) = O_p(1)\]
    By Slutsky's theorem:
    \[\sqrt{n}(\widehat{\text{ATE}}_{\text{RATE}} - \text{ATE}) = O_p(1)\]
\end{proof}

% reference thesis.bib
% citation style: abbreviated standard
\newpage
\bibliographystyle{abbrv}
\bibliography{thesis}

\end{document}

\end{\end{filecontents}}

\begin{document}

\subfile{cover}

\subfile{body}

\end{document}
